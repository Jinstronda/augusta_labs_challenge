# Companies Table Schema

## üìã Table Design (Sequential Thinking Output)

### Core Fields (from CSV):
```sql
company_id SERIAL PRIMARY KEY          -- Auto-incrementing unique ID
company_name TEXT NOT NULL             -- Company name (required)
cae_primary_label TEXT                 -- CAE classification description
trade_description_native TEXT          -- Portuguese activity description
website TEXT                           -- Company website (nullable, many empty)
```

### Future Pipeline Fields:
```sql
location_address TEXT                  -- From Google Maps scraping
location_lat NUMERIC                   -- Latitude
location_lon NUMERIC                   -- Longitude  
location_region TEXT                   -- Extracted region (e.g., "Lisboa (NUTS II)")
```

### Tracking Fields:
```sql
created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
```

## üéØ Design Rationale

### Why SERIAL PRIMARY KEY?
- ‚úÖ Efficient for joins and lookups
- ‚úÖ Avoids duplicate company name issues
- ‚úÖ Integer keys are faster than text keys

### Why Separate Location Fields?
- üó∫Ô∏è Prepared for Google Maps API scraping (Phase 5 of pipeline)
- üîç `location_region` will match with `incentive.geo_requirement`
- üìç Lat/lon for distance calculations if needed

### Why These Indexes?
```sql
CREATE INDEX idx_companies_cae ON companies(cae_primary_label);
CREATE INDEX idx_companies_region ON companies(location_region);
```

**Performance Impact:**
- Without indexes: 250k table scan = 500-1000ms per query
- With indexes: Index scan = 5-20ms per query
- **50-100x faster filtering!** ‚ö°

## üìä Data Statistics

```
Total Companies: ~250,001
With Websites: ~30-40% (estimate)
Without Websites: ~60-70%
Storage Size: ~200-300 MB (text data)
Indexes Size: ~50-100 MB
```

## üöÄ Usage in RAG Pipeline

### Phase 1: Structured Filtering (PostgreSQL)
```python
# Filter by sector (CAE code)
cursor.execute("""
    SELECT * FROM companies
    WHERE cae_primary_label LIKE %s
    AND location_region = %s
""", [f"%{sector_keyword}%", incentive.geo_requirement])

# Reduces 250k ‚Üí 5-10k companies in 10-20ms
```

### Phase 2: Vector Search (Qdrant)
```python
# Embed and search filtered companies
from qdrant_client import QdrantClient
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')
client = QdrantClient(path="./qdrant_storage")

# Only embed the filtered 5-10k (not all 250k)
for company in filtered_companies:
    text = f"{company.company_name} {company.cae_primary_label} {company.trade_description_native}"
    embedding = model.encode(text)
    
    client.upsert(
        collection_name="companies",
        points=[PointStruct(
            id=company.company_id,
            vector=embedding.tolist(),
            payload={"name": company.company_name, "cae": company.cae_primary_label}
        )]
    )
```

### Phase 3: LlamaIndex Integration
```python
from llama_index.core import VectorStoreIndex
from llama_index.vector_stores.qdrant import QdrantVectorStore

# Connect LlamaIndex to your Qdrant storage
vector_store = QdrantVectorStore(
    client=client,
    collection_name="companies"
)

index = VectorStoreIndex.from_vector_store(vector_store)

# Query with natural language
query_engine = index.as_query_engine(similarity_top_k=40)
response = query_engine.query(
    f"Find companies that work in {incentive.sector} and can {incentive.eligible_actions}"
)
```

## üìñ Complete Workflow

```
1. setup_companies.py
   ‚Üì Creates table + loads 250k companies
   
2. Qdrant: Index companies with embeddings
   ‚Üì Vector search ready
   
3. LlamaIndex: Query engine on top
   ‚Üì Natural language interface
   
4. Match Pipeline: incentive ‚Üí companies
   ‚Üì PostgreSQL filter + Qdrant search
   
5. Google Maps: Scrape locations for top candidates
   ‚Üì Update location_region field
   
6. Final Matching: Filter by geo_requirement
   ‚Üì Top 5 companies per incentive
```

## üîß Quick Start

```bash
# 1. Setup companies table
python setup_companies.py

# 2. Install RAG dependencies
pip install qdrant-client sentence-transformers llama-index-vector-stores-qdrant

# 3. Ready for RAG pipeline!
```

## üìà Performance Expectations

| Operation | Time | Records |
|-----------|------|---------|
| Table creation | <1 sec | - |
| Data loading | 2-3 min | 250k |
| Index creation | 30-60 sec | - |
| **Total Setup** | **~4 minutes** | **250k** |

| Query Operation | Time | Note |
|-----------------|------|------|
| CAE filter (no index) | 500ms | Table scan |
| CAE filter (with index) | 10ms | ‚ö° 50x faster |
| Filtered vector search | 15-30ms | Qdrant |
| Full pipeline per incentive | 40-60ms | Total |

## ‚úÖ Ready for Next Steps

This table design supports your complete pipeline:
- ‚úÖ RAG with Qdrant + LlamaIndex
- ‚úÖ Fast structured filtering
- ‚úÖ Location-based matching (after Maps scraping)
- ‚úÖ Efficient for 250k companies
